{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import function_to_test as test\n",
    "import utilities as util\n",
    "from user_variables import attr_to_change, attr_with_max_dom, attr_relaxed_privacy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program execution module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_execution_module(data, k, number_of_conditions):\n",
    "\n",
    "    buckets = {}\n",
    "    \"\"\" For each point i in the dataset call test_program(i) and update the bucket dictionary.\n",
    "        Sample tuple: {tuples: [tuple1, ...., tupleN], \n",
    "                       constraints: [[attr, lambda, taken],...,]} \"\"\"\n",
    "    for i in data:\n",
    "\n",
    "        path_condition_full = [] # Contains the set of mandatory conditions for the constraint solver\n",
    "        for j in range (0, number_of_conditions):\n",
    "            path_condition_full.append([-1, None, 0])\n",
    "        path_condition_full = test.test_program(i, path_condition_full)\n",
    "        path_condition = util.list_to_string(path_condition_full)\n",
    "\n",
    "        try:\n",
    "            buckets[path_condition][\"tuples\"].append(i)\n",
    "        except KeyError:\n",
    "            buckets[path_condition] = {}\n",
    "            buckets[path_condition][\"tuples\"] = []\n",
    "            buckets[path_condition][\"constraints\"] = path_condition_full\n",
    "            buckets[path_condition][\"tuples\"].append(i)\n",
    "\n",
    "            \n",
    "    # Remove buckets with less than k tuples\n",
    "    to_remove = []\n",
    "    for key in buckets:\n",
    "        if (len(buckets[key][\"tuples\"]) < k):\n",
    "            to_remove.append(key)\n",
    "\n",
    "    for i in to_remove:\n",
    "        del buckets[i]\n",
    "    \n",
    "    # Print all the covered branches for debug pourpose\n",
    "    print(\"Paths:\\n\")\n",
    "    for entry in buckets:\n",
    "        print(entry)\n",
    "\n",
    "    return buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costraint generation and solver module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_and_data_generation_module(data, buckets, tuple_length, option):\n",
    "    \n",
    "    log = open(\"log\", \"w\")\n",
    "    \n",
    "    # Same path no tuple repeat\n",
    "    if (option == \"pt\"):\n",
    "        new_data = []\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for pc in buckets:\n",
    "                buckets[pc][\"constraints\"].extend(util.same_path_no_tuple_repeat(buckets[pc][\"tuples\"]))\n",
    "                new_data.append(constraint_solver(buckets[pc][\"constraints\"], tuple_length))\n",
    "\n",
    "            return new_data\n",
    "        \n",
    "        except util.CardinalityException as e:\n",
    "            \n",
    "            raise Exception(\"Cannot have same values for the attribute \", + str(e.attr) + \". No tuple repeat cannot be fullfilled\")\n",
    "    \n",
    "    \n",
    "    # Same path no field repeat\n",
    "    if (option == \"pf\"):\n",
    "        \n",
    "        global_constraints = util.same_path_no_field_repeat(data)\n",
    "        new_data = []\n",
    "        up_to = 0   # Number of currently generated tuples\n",
    "        \n",
    "        # Create some (deep)copies \n",
    "        old_buckets = copy.deepcopy(buckets) \n",
    "        old_constraints = copy.deepcopy(global_constraints)\n",
    "        while(True):\n",
    "            \n",
    "            buckets = copy.deepcopy(old_buckets)   # In order not to extend with the same values many times\n",
    "            try:\n",
    "                for k in range(up_to, len(buckets)):\n",
    "                    pc = list(buckets)[k]\n",
    "                    buckets[pc][\"constraints\"].extend(global_constraints)\n",
    "                    new_data.append(constraint_solver(buckets[pc][\"constraints\"], tuple_length))\n",
    "                    up_to += 1\n",
    "                    \n",
    "                    # Recover old configuration in order to relax constraints \n",
    "                    # only for tuples that raised an exception\n",
    "                    global_constraints = copy.deepcopy(old_constraints) \n",
    "                    \n",
    "                return new_data\n",
    "    \n",
    "            except util.CardinalityException as e:\n",
    "                \n",
    "                # Writing to log gives an estimation of how much privacy was relaxed\n",
    "                log.write(\"Cardinality Exception on attribute \" + str(e.attr))\n",
    "                \n",
    "                if(attr_relaxed_privacy[str(e.attr)] == 0):\n",
    "                    if (attr_with_max_dom[\"attr\"] == e.attr):\n",
    "                        # Unavoidable exception that guarantees at least no tuple repeat\n",
    "                        raise Exception(\"Cannot have relaxed privacy on attribute with highest cardinality. No tuple repeat will not be fullfilled\")\n",
    "                    # Unavoidable exception if the user is not willing to sacrifice privacy on certain attributes\n",
    "                    raise Exception(\"Cannot have relaxed privacy on attribute \" + str(e.attr))\n",
    "                    \n",
    "                # Relax privacy on the selected attribute    \n",
    "                util.remove_constraints(global_constraints, e.attr)\n",
    "            \n",
    "        \n",
    "    raise Exception(\"Invalid Option\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def constraint_solver(constraints, tuple_length):\n",
    "    new_t = [0] * tuple_length\n",
    "    for i in range(0, tuple_length):\n",
    "        generated_values = []\n",
    "        # Select only the constraints for the desired attribute\n",
    "        temp_const = util.get_constraints(i, constraints)\n",
    "        # Generate a new value according to user_variables parameters\n",
    "        temp_val = util.gen_value(i, generated_values)\n",
    "        # Check if the constraints hold for the new value\n",
    "        temp_res = util.check_constraints(temp_val, temp_const, i)\n",
    "        num_tries = 1\n",
    "        \n",
    "        while (not(temp_res[0])):\n",
    "            num_tries+=1\n",
    "            \n",
    "            if (num_tries >= attr_with_max_dom[\"dom_card\"]):\n",
    "                # Special case if the user wants to manually limit the number of tries \n",
    "                # instead of choosing the attribute whose domain has the highest cardinality\n",
    "                raise Exception('Can\\'t generate tuple: maximum number of tries has been reached while generating: ' + str(i))\n",
    "            # Repeat the same procedure until a correct value is generated\n",
    "            temp_val = util.gen_value(i, generated_values)\n",
    "            temp_res = util.check_constraints(temp_val, temp_const, i)\n",
    "            \n",
    "        new_t[i] = temp_res[1]\n",
    "        \n",
    "    return new_t  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
